# THis is config for data generate file
config_path : ./utils/config.yaml 


# dataset
dataset: sent140

# number of Clients (default n=5) 
clients: 100
# client port (which we will start)
client_port: 8000

# reseve data for the server
server_data: false

# split mode dataset split: iid , niid
split_mode: niid

# ---- split_mode: iid -----

# share samples between clients in the iid split mode
iid_share: false

# percentage of samples to share between clients 
iid_rate: 0.1

# ---- split_mode: niid -----

# the number of samples that hold each client 
# if server_data is true you should indicate the size of the data server in the last index (Ex : [1822, 1000, 1200, 1300, 1000, 2314])
#data_size: [1822, 1000, 1200, 1300, 1000]
data_size: [1728,  532,  662, 1692, 1072,  986, 1527,  506,  405, 1396, 1129,
        317, 1302, 1256,  680, 1367, 1391,  233,  128, 1311, 1333,    7,
        237, 1068,  753,  489,  943, 1443, 1673, 1267, 1503, 1320, 1496,
        177, 1055,  869, 1680,  267,  919, 1492,  410,  720,  966,  534,
        566, 1233,  503,  526,  918, 1270, 1314,   90, 1659, 1182, 1364,
       1462,  641, 1690,  230,  807, 1148, 1439,  363,  974, 1421, 1658,
        165,  559, 1750, 1174, 1598,  976, 1576, 1485,  318, 1091, 1124,
       1542, 1645,  205, 1397, 1370, 1505,  226, 1153, 1758,  420,  423,
       1154,  194, 1713,  463,  591, 1391,  739,  925, 1449,  685,  632,
       1905]

# there are 2 type : 'random' and 'label' for split by label 
type: label

# ---- type: label -----

# number of class in each client 
label_num: [1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1,
       1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1,
       2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2,
       1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1,
       1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1]

# how to share samples between clients who hold the same classes:
  # 0: Share the same samples of class 
  # 1: Share samples of class randomly 
  # 2: Share different samples of class 
  # 3: Share different class (sum(label_num) must <= number of class)
share_samples: 1

# share global dataset to all clients
global_dataset: false

# percentage of samples in the global dataset which we want to add
data_rate: 0.1

# if we want to add error to some samples 
add_error: false
# percentage of error to add
error_rate: 0.01

# the training config

# batch size of the training
batch_size: 1
# batch size used for the test data
test_batch_size: 128
# number of federated learning rounds
training_rounds: 2000
# number of training steps performed on each remote worker before averaging
federate_after_n_batches: 1
# learning rate
lr: 0.1
#max number of batch
max_nr_batches: 3
# use cuda
cuda: true
# seed used for randomization
seed: 1
# if set, model will be saved
save_model: false
# if set, websocket client workers will be started in verbose mode
verbose: false
# Evaluate the model evrey n rounds
eval_every: 2
# Number of clients that will in each round
fraction_client: 0.1
# the optimazer that we will use : SGD or Adam
optimizer: SGD
# The model that we will use 
model: lstm
# type of aggragation : federated_avg or wieghted_avg
aggregation: federated_avg
# loss function : nll_loss or cross_entropy
loss: cross_entropy




