# THis is config for data generate file
config_path : ./utils/config.yaml 


# dataset
dataset: fashionmnist

# number of Clients (default n=5) 
clients: 100
# client port (which we will start)
client_port: 8000

# reseve data for the server
server_data: false

# split mode dataset split: iid , niid
split_mode: niid

# ---- split_mode: iid -----

# share samples between clients in the iid split mode
iid_share: false

# percentage of samples to share between clients 
iid_rate: 0.1

# ---- split_mode: niid -----

# the number of samples that hold each client 
# if server_data is true you should indicate the size of the data server in the last index (Ex : [1822, 1000, 1200, 1300, 1000, 2314])
#data_size: [1822, 1000, 1200, 1300, 1000]
data_size: [ 798, 1111,  865,  323,  904, 1135,  656,  839,   88,  276,  829,
        442,  366,  193, 1116,   50,   50, 1066,  680,  145, 1006,  332,
        839,  637,  232,  521,  893,  565,  361,  355,  344,  728,  896,
        344,  347,  724,  868,  479,   97,  763,  437,  114, 1125,   72,
        957,  326,  410, 1057,  632,  176,  744, 1015,  634,  163,   12,
        981,  242,  468,  583,  938, 1104,  358,  244,  951,  703,  550,
        712, 1103,  134,  884, 1087,  861,  506,  325,  711,  468,  242,
        183,  971,  543,  745,  175,  147, 1072, 1020,  526,  756,  623,
        632,  557,  483, 1005, 1065,  474,  473,  328, 1162,  827,  124,
        817]

# there are 2 type : 'random' and 'label' for split by label 
type: random

# ---- type: label -----

# number of class in each client 
label_num: [ 7,  8,  6,  8,  8,  2, 10,  1,  4,  2,  3,  5,  4,  4,  9, 10,  9,
        1,  2,  9,  4,  7,  3,  6, 10,  9,  8,  6,  6,  1,  3, 10, 10,  7,
        9, 10,  6,  8,  9,  2,  7,  3,  3,  4,  7,  6, 10, 10, 10,  9,  6,
        6,  1,  5, 10,  8,  7,  2,  4,  5, 10,  9,  7,  9,  5,  1,  1,  4,
        8,  9,  4,  2,  6,  1,  1,  2,  2,  2,  5,  5,  4,  8,  3,  3,  9,
        8, 10,  5, 10,  3,  2,  7,  9,  8,  6,  9,  3,  3,  6,  5]

# how to share samples between clients who hold the same classes:
  # 0: Share the same samples of class 
  # 1: Share samples of class randomly 
  # 2: Share different samples of class 
  # 3: Share different class (sum(label_num) must <= number of class)
share_samples: 0

# share global dataset to all clients
global_dataset: false

# percentage of samples in the global dataset which we want to add
data_rate: 0.1

# if we want to add error to some samples 
add_error: false
# percentage of error to add
error_rate: 0.01

# the training config

# batch size of the training
batch_size: 10
# batch size used for the test data
test_batch_size: 128
# number of federated learning rounds
training_rounds: 50
# number of training steps performed on each remote worker before averaging
federate_after_n_batches: 1
# learning rate
lr: 0.1
#max number of batch
max_nr_batches: 10
# use cuda
cuda: true
# seed used for randomization
seed: 1
# if set, model will be saved
save_model: false
# if set, websocket client workers will be started in verbose mode
verbose: false
# Evaluate the model evrey n rounds
eval_every: 2
# Number of clients that will in each round
fraction_client: 0.1
# the optimazer that we will use : SGD or Adam
optimizer: SGD
# The model that we will use 
model: lstm
# type of aggragation : federated_avg or wieghted_avg
aggregation: federated_avg
# loss function : nll_loss or cross_entropy
loss: cross_entropy


